<!DOCTYPE html>
<html>
<meta charset="utf-8">
<link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
<link href="html/build/nv.d3.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="html/css/style.css">
<link href="html/js/c3.css" rel="stylesheet" type="text/css">

<body>
	<div class="title">
		<h1 class="text-center">state-of-states</h1>

		<p class="lead text-center">Finding correlations between <strong>Wikipedia</strong> data and a country’s <strong>economic success</strong></p>

		<p class="text-center">Many of us turn to Wikipedia everyday for knowledge about the world, yet what influences the content which is available?</br>by <strong>Harrison Pincket</strong> and <strong>Indira Pranabudi</strong></p>
	</div>

	<div class="normal">
		<h1>Introduction</h1>
		<p>We hypothesize that the amount of <strong>attention</strong> a country receives on <strong>Wikipedia</strong> is positively correlated to that country’s <strong>economic success</strong>. In this way our knowledge of different areas of the world is indirectly influenced by the global economy.</p>

		<p>A country’s attention on Wikipedia can be defined in many ways. In this project, it is defined as a combination of:</p>

		<ul>
			<li>Number of edits.</li>
			<li>Number of forward links.</li>
			<li>Page size.</li>
			<li>Number of citations.</li>
		</ul>

		<p>Meanwhile, a country’s economic success is defined by its GDP.</p>
	</div>

	<div class="normal">
		<h1>Data</h1>

		<p>We used two primary sources of data:</p>

		<h3>Wikipedia Pages</h3>

		<p>There were several different options we could use to determine whether or not a pge is related to a particular country:</p>

		<ul>
			<li>Pages with the country name in the title. <a target="_blank" href="http://en.wikipedia.org/w/index.php?search=brazil&title=Special%3ASearch&fulltext=1">Here</a> is the list of pages with the term "Brazil" in its title.</li>
			<li>Portal pages. <a target="_blank" href="http://en.wikipedia.org/wiki/Portal:Brazil">Here</a> is the portal page for Brazil.</li>
			<li>Category pages. <a target="_blank" href="http://en.wikipedia.org/wiki/Category:Brazil">Here</a> is the Category page for Brazil.</li>
			<li>Index pages. <a target="_blank" href="https://www.wikiwand.com/en/Index_of_Brazil-related_articles">Here</a> is the page for "Index of Brazil-related pages".</li>
			<li>Outline of country pages. <a href="http://en.wikipedia.org/wiki/Outline_of_Brazil">Here</a> is the "Outline of Brazil" page.</li>
			<li>Related topics listed at the bottom of a country's page.</li>
		</ul>

		<p>In the end, we decided to use the Category pages at it seemed to be the most up-to-date and standardized list among the others.</p>

		<p>We also scraped past revisions of each page to provide us with data to compare to past GDP values. At the top right section of a Wikipedia page, there is a tab called <a target="_blank" href="http://en.wikipedia.org/w/index.php?title=Brazil&action=history">"View history"</a> which lets us view past revisions. We wrote a Python script to dowload one revision (the latest one) for each month since the page was created. Downloading all of the past revisions would give us too many files.</p>

		<p>In total, we downloaded <strong>1,634,173 pages</strong>, which take up <strong>116GB</strong> and took approximately <strong>1 week</strong> to download.

		<img src="html/img/View history.png">

		<h3>World Bank Data</h3>

		<p><a href="http://data.worldbank.org/">World Bank statistics</a>, which contains GDP data along with other data such as life expectancy, poverty rate and school enrollment rate. World Bank provides these information in several different formats, including CSV format. These demographics from the World Bank have already been collected and cleaned.</p>
	</div>

	<div class="normal">
		<h1>Methodology</h1>

		<h3>Collecting Data</h3>

		<p>We scraped the Wikipedia pages using a Python library called <strong><a target="_blank" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a></strong>. We ran <code><a target="_blank" href="src/extract_states.py">extract_states.py</a></code> to first extract the list of countries on Wikipedia, and then <code><a target="_blank" href="src/category_downloader.py">category_downloader.py</a></code> to extract a list of the associated pages through the Category page.</p>

		<p>We stored the Wikipedia articles in plain text and ran <code><a target="_blank" href="src/wiki_attention.py">wiki_attention.py</a></code>extracted the desired features locally, and store them in CSV files.</p>

		<p>The original goal for this project is to use data from 25 different countries. However, upon writing the Python scripts, we realized that we could download data from all the countries listed on Wikipedia.</p>

		<h3>Timeline</h3>

		<table class="table table-hover">
			<tr>
				<td>Task</td>
				<td>Hours</td>
				<td>Deadline</td>
			</tr>
			<tr>
				<td>Determine pages to scrape</td>
				<td>2</td>
				<td>April 5th</td>
			</tr>
			<tr>
				<td>Identify a process to scrape past revisions</td>
				<td>4</td>
				<td>April 9th</td>
			</tr>
			<tr>
				<td>Scrape the pages</td>
				<td>3</td>
				<td>April 12th</td>
			</tr>
			<tr>
				<td>Extract features from scraped pages and convert to CSV</td>
				<td>4</td>
				<td>April 18th</td>
			</tr>
			<tr>
				<td>Apply machine learning to extracted features and demographics. Tweak. Repeat.</td>
				<td>14</td>
				<td>April 30th</td>
			</tr>
			<tr>
				<td>Visualize (depending on findings)</td>
				<td>3-6</td>
				<td>May 2nd</td>
			</tr>
			<tr>
				<td>Write-up</td>
				<td>4</td>
				<td>May 5th</td>
			</tr>
			<tr>
				<td>Design and Print Poster</td>
				<td>3</td>
				<td>May 7th</td>
			</tr>
		</table>
	</div>

	<div class="normal">
		<h1>Deliverables</h1>

		<ul>
			<li>75%: Poor analysis of data using machine learning of only an insufficient number of countries. Boring, standard visualizations</li>
			<li>100%: Great analysis of the data as outlined above, using 25 different countries and a decent set of visualizations.</li>
			<li>125%: Study of 100 countries. Further analysis including modeling based on geographic regions. Elaborate visualizations, perhaps including advanced map projections to demonstrate regional differences in the influence on Wikipedia.</li>
		</ul>
	</div>

	<div class="normal">
		<h1>Backup Plan</h1>

		<p>In case we fail to discover a correlation between Wikipedia attention and the economic success of a country, we will report the negative results, and attempt to find a correlation with a different demographic listed on the World Bank database.</p>
	</div>

	<div class="normal">
		<h1>Findings</h1>

		<p>The first thing we did was to plot the number of citations, forward links and file sizes of a particular country over time, and compare it to the GDP of that country. Although we found that generally, a country with a high GDP also has a high Wikipedia attention, we found that this does not mean a sharp decrease in GDP affect the Wikipedia attention in the same way (i.e. decrease the amount of Wikipedia attention). It is very unlikely that the number of pages related to a country would decrease, or that the length of the article would decrease.</p>

		<p>Looking at the graph of GDP vs. average file size of pages related to the USA shows that there is generally a positive correlation between the two metrics:</p>
		<div class="chart-usa"></div>

		<p>However, looking at the graph of GDP vs. average file size of pages related to Greece shows that the relationship is not as straightforward as we thought it would be:</p>
		<div class="chart-greece"></div>

		<p>We then decided to visualize the number of edits per country on a world map:</p>

		<div id="map" max-width="100%">
			<div id="tooltip" class="hidden">
				<p><strong><span id="value">100</span></strong></p>
			</div>
		</div>

		<div id="legend"></div>

		<p>We also created a scatter plot of all the countries, to compare its GDP with its number of edits:</p>

		<img src="html/img/scatter.png">		

		<p>The countries with the most number of page edits are:</p>

		<ul>
			<li>United Kingdom - 36671</li>
			<li>United States - 26782</li>
			<li>India - 23635</li>
			<li>Israel - 23414</li>
			<li>Japan - 22629</li>
		</ul>

		<p>We also calculated the Pearson correlation between all the GDP metrics and the countries' number of citations, file sizes as well as number of forward links. Here are the metrics with the highest Pearson correlation:</p>

		<ul>
			<li>Citations with GDP (2005 US$): 0.724</li>
			<li>Citations with GNI, Atlas method (current US$): 0.707</li>
			<li>Citations with GNI (current US$): 0.7066</li>
			<li>Citations with GDP (current US$): 0.7061</li>
			<li>Citations with number of secure Internet servers: 0.681</li>
		</ul>

		<p>Meanwhile, the metrics with the lowest (absolute) Pearson correlation are:</p>

		<ul>
			<li>File size with freshwater withdrawal: -0.000997</li>
			<li>File size with ratio of women in ministry: 0.00225</li>
			<li>Links with freshwater withdrawal: -0.00522</li>
			<li>Links with pre-primary entrance age: 0.00753</li>
			<li>Citations with freshwater withdrawal: 0.00891</li>
		</ul>
	</div>

	<div class="normal">
		<h1>Challenges</h1>

		<h3>Data Collection</h3>

		<p>Determining which pages were associated with a country was harder than we thought. As stated earlier in the Data section, there were multiple options we could use. Ultimately, we decided that it would be best for us to use the Categories page as it was the most standardized and up-to-date list.</p>

		<p>The next challenge was deciding how deep we should look into the Categories page, as each category had subcategories, each subcategory and its own subcategories, and so on. We had to regulate our search depth because categories and subcategories tended to overlap, leading to unrelated pages. Moreover, we also had to regulate the search depth in order to reduce the volume of data.</p>

		<p>Here is an example of some pages associated with Austria to demonstrate how wide our search is:</p>

		<ul>
			<li>Grand Duchy of Tuscany (<a target="_blank" href="http://en.wikipedia.org/wiki/Grand_Duchy_of_Tuscany">link</a>)</li>
			<li>Vienna Summer of Logic (<a target="_blank" href="http://en.wikipedia.org/wiki/Vienna_Summer_of_Logic">link</a>)</li>
			<li>Praetorian prefecture of Illyricum (<a target="_blank" href="http://en.wikipedia.org/wiki/Praetorian_prefecture_of_Illyricum">link</a>)</li>
		</ul>

		<p>Here are some pages associated with Sierra Leone:</p>

		<ul>
			<li>Armed Forces Revolutionary Council (<a target="_blank" href="http://en.wikipedia.org/wiki/Armed_Forces_Revolutionary_Council">link</a>)</li>
			<li>Parliament of Sierra Leone (<a href="http://en.wikipedia.org/wiki/Parliament_of_Sierra_Leone">link</a>)</li>
			<li>Islam in Sierra Leone (<a href="http://en.wikipedia.org/wiki/Islam_in_Sierra_Leone">link</a>)</li>
		</ul>

		<p>We also had to handle 404 and 500 Errors.</p>

		<h3>Sparse Data</h3>

		<p>Not all Wikipedia pages have been around since 2001, and most Wikipedia pages aren't updated every month. Therefore, when we first tried to project the number of edits on a Wikipedia page on a matrix, the dataset was very sparse! We had to transform the data we had into a dense matrix representation before we could do some data crunching on the dataset.</p>

		<h3>Entity Resolution</h3>

		<p>We have two primary sources of data—-Wikipedia and the World Bank. However, there are several countries recognized by Wikipedia but not by the World Bank (such as Somaliland, Nagorno-Karabakh Republic, and South Ossetia), and vice versa (St. Martin, Sint Maarten, and Macao are some examples). Moreover, the names of different countries are listed differently on different sources. For instance, Macedonia is known as the Republic of Macedonia on Wikipedia, but Macedonia, FYR on the World Bank. North Korea is known as North Korea on Wikipedia, but Korea, Dem. Rep. on the World Bank.</p>
	</div>
	
	<script src="http://d3js.org/d3.v3.min.js" charset="utf-8"></script>
	<script src="html/js/c3.min.js"></script>

	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.2/d3.min.js" charset="utf-8"></script>
	<script src="html/build/nv.d3.js"></script>

	<script src="http://d3js.org/d3.geo.projection.v0.min.js"></script>
	<script src="http://d3js.org/topojson.v1.min.js"></script>

	<script src="http://d3js.org/queue.v1.min.js"></script>
	<script src="http://labratrevenge.com/d3-tip/javascripts/d3.tip.v0.6.3.js"></script>
	<script>

	var width = 960,
		height = 580;

	var colors = ["#ECF9D8", "#CFF09E", "#A8DBA8", "#79BD9A", "#6DAA8B", "#3B8686", "#0B486B", "#093A56"];
	// var intervals = [0.00018011, 0.0003363, 0.0005137, 0.0008364, 0.001467, 0.003209, 0.012256]; // edits per population
	// var intervals = [255009, 969725, 2057170, 4445249, 8395512, 19092981, 56532728]; // gdp per edits
	var intervals = [3000, 4000, 5021, 6569, 8541, 11571, 14454];

	// var colors = ["red", "blue"]
	// var intervals = [10000000]

	var color = d3.scale.threshold()
		.domain(intervals)
		.range(colors);

	var projection = d3.geo.kavrayskiy7()
			.scale(170)
			.translate([width / 2, height / 2])
			.precision(.1);

	var path = d3.geo.path()
			.projection(projection);

	var graticule = d3.geo.graticule();

	var svg = d3.select("#map").append("svg")
			.attr("width", width)
			.attr("height", height);

	svg.append("defs").append("path")
			.datum({type: "mercator"})
			.attr("id", "sphere")
			.attr("d", path);

	svg.append("use")
			.attr("class", "stroke")
			.attr("xlink:href", "#sphere");

	svg.append("use")
			.attr("class", "fill")
			.attr("xlink:href", "#sphere");

	var legend = d3.select("#legend").append("svg")
		.data(["#ECF9D8", "#CFF09E", "#A8DBA8", "#79BD9A", "#3B8686", "#0B486B", "#093A56"])
		.attr("width", width)
		.attr("height", 70);

	queue()
		.defer(d3.json, "html/vis/world-50m.json")
		.defer(d3.tsv, "html/vis/data.tsv")
		.await(ready);

	function ready(error, world, countrycodes) {
		var rateById = {};
		var getName = {};

		countrycodes.forEach(function(d) {
			rateById[d.id] = d.edits;
			getName[d.id] = d.country;
		});

		var countries = topojson.feature(world, world.objects.countries).features,
			neighbors = topojson.neighbors(world.objects.countries.geometries);

		svg.selectAll(".country")
				.data(countries)
			.enter().insert("path", ".graticule")
				.attr("class", "country")
				.attr("d", path)
				.style("fill", function(d) { return color(rateById[d.id]); })
				.on('mouseover', function(d, i) {
					d3.select(this).style("fill", "#bcbcbc");

					//Get this bar's x/y values, then augment for the tooltip
					var xPosition = d3.event.pageX;
					var yPosition = d3.event.pageY;

					//Update the tooltip position and value
					d3.select("#tooltip")
					  .style("left", (xPosition - 60) + "px")
					  .style("top", (yPosition - 30) + "px")
					  .select("#value")
					  .text(function() {
						if (getName[d.id] == "") {
							return "N/A";
						}
						if (rateById[d.id] == "") {
							return getName[d.id] + " - data N/A";
						}
						return getName[d.id] + " - " + rateById[d.id] + " edits";
					  });

					d3.select("#tooltip").classed("hidden", false);
					})
				.on('mouseout', function(d) {
					d3.select(this).style("fill", function(d) { return color(rateById[d.id]); });

					d3.select("#tooltip").classed("hidden", true);
				});

		svg.insert("path", ".graticule")
			.datum(topojson.mesh(world, world.objects.countries, function(a, b) { return a !== b; }))
			.attr("class", "boundary")
			.attr("d", path);

		legend.append("rect")
			.attr("x", 0)
			.attr("y", 10)
			.attr("width", (800/7))
			.attr("height", 20)
			.style("fill", "#ECF9D8");

		legend.append("rect")
			.attr("x", (800/7))
			.attr("y", 10)
			.attr("width", (800/7))
			.attr("height", 20)
			.style("fill", "#CFF09E");

		legend.append("rect")
			.attr("x", (800/7*2))
			.attr("y", 10)
			.attr("width", (800/7))
			.attr("height", 20)
			.style("fill", "#A8DBA8");

		legend.append("rect")
			.attr("x", (800/7*3))
			.attr("y", 10)
			.attr("width", (800/7))
			.attr("height", 20)
			.style("fill", "#79BD9A");

		legend.append("rect")
			.attr("x", (800/7*4))
			.attr("y", 10)
			.attr("width", (800/7))
			.attr("height", 20)
			.style("fill", "#3B8686");

		legend.append("rect")
			.attr("x", (800/7*5))
			.attr("y", 10)
			.attr("width", (800/7))
			.attr("height", 20)
			.style("fill", "#0B486B");

		legend.append("rect")
			.attr("x", (800/7*6))
			.attr("y", 10)
			.attr("width", (800/7))
			.attr("height", 20)
			.style("fill", "#093A56");

		var intervals = [3000, 4000, 5021, 6569, 8541, 11571, 14454];

		legend.append("text")
			.text("≥ 3000 edits")
			.attr("font-family", "@font-family-base")
			.attr("font-size", "9px")
			.attr("x", 10)
			.attr("y", 40);

		legend.append("text")
			.text("≥ 4000 edits")
			.attr("font-family", "@font-family-base")
			.attr("font-size", "9px")
			.attr("x", (800/7) + 10)
			.attr("y", 40);

		legend.append("text")
			.text("≥ 5021 edits")
			.attr("font-family", "@font-family-base")
			.attr("font-size", "9px")
			.attr("x", (800/7*2) + 10)
			.attr("y", 40);

		legend.append("text")
			.text("≥ 6569 edits")
			.attr("font-family", "@font-family-base")
			.attr("font-size", "9px")
			.attr("x", (800/7*3) + 10)
			.attr("y", 40);

		legend.append("text")
			.text("≥ 8541 edits")
			.attr("font-family", "@font-family-base")
			.attr("font-size", "9px")
			.attr("x", (800/7*4) + 10)
			.attr("y", 40);

		legend.append("text")
			.text("≥ 11571 edits")
			.attr("font-family", "@font-family-base")
			.attr("font-size", "9px")
			.attr("x", (800/7*5) + 10)
			.attr("y", 40);

		legend.append("text")
			.text("≥ 14454 edits")
			.attr("font-family", "@font-family-base")
			.attr("font-size", "9px")
			.attr("x", (800/7*6) + 10)
			.attr("y", 40);	
	}

	</script>

	<script>
	var chart = c3.generate({
		bindto: '.chart-greece',
		color: {
		  pattern: ['#CFF09E', '#3B8686']
		},
		data: {
			x: 'x',
		  columns: [
			['x', 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013],
			['File Size', 44478, 38068, 39665, 41367, 44124, 49240, 56255, 62617, 68816, 74452, 80173, 85495, 89335],
			['GDP', 212611721758, 219335821022, 233894807367, 245478738870, 247665771278, 262068232726, 271338832139, 270133049794, 258260896384, 244189836218, 222545123738, 207919209322, 201025631249]
		  ],
		  axes: {
			GDP: 'y2' // ADD
		  }
		},
		axis: {
		  y: {
			label: { // ADD
			  text: 'Average File Size',
			  position: 'outer-middle'
			}
		  },
		  y2: {
			show: true,
			label: { // ADD
			  text: 'GDP (2005 US$)',
			  position: 'outer-middle'
			},
			tick: {
			  format: d3.format("$,") // ADD
			}
		  }
		}
		});
	</script>

	<script>
	var chart = c3.generate({
		bindto: '.chart-usa',
		color: {
		  pattern: ['#CFF09E', '#3B8686']
		},
		data: {
			x: 'x',
		  columns: [
			['x', 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013],
			['File Size', 55753, 59632, 60963, 63804, 68797, 79278, 86238, 96551, 103889, 112979, 116718, 123993, 132717],
			['GDP', 10621800000000, 10977500000000, 11510700000000, 12274900000000, 13093700000000, 13855900000000, 14477600000000, 14718600000000, 14418700000000, 14964400000000, 15517900000000, 16163200000000, 16768100000000]
		  ],
		  axes: {
			GDP: 'y2' // ADD
		  }
		},
		axis: {
		  y: {
			label: { // ADD
			  text: 'Average File Size',
			  position: 'outer-middle'
			}
		  },
		  y2: {
			show: true,
			label: { // ADD
			  text: 'GDP (2005 US$)',
			  position: 'outer-middle'
			},
			tick: {
			  format: d3.format("$,") // ADD
			}
		  }
		}
		});
	</script>
</body>
</html>